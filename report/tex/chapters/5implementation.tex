\section{Design \& Implementation}\label{chapter.implementation}
\thispagestyle{plain}

\comment{[Backend/Off-Stage]}

This chapter aims to explain how \toolname \space is designed and implemented. \comment{It starts with a visualization and narrative of how data flows through the system.} It \comment{then} provides a description of how the system utilizes Selenium Grid. The implementation of the controller and some important details about this part of the system are then presented. Further, the test allocation mechanism called \emph{OptiX} is explained along with an accompanying example to illustrate how the mechanism works step-by-step. The chapter then describes how the database is structured and explains how it is accessed using Django's API for database abstraction, before finally presenting some important implementation details about the dashboard.

\subsection{Selenium Grid Integration}\label{section.selenium}

Selenium Grid works as the backbone of all interaction between the server and the remaining machines in the distributed system of \toolname. It is used to establish connections and to send browser calls to test machines upon test executions, as explained in Section \ref{subsubsec.seleniumGrid}.

A Selenium Grid hub can be started simply by executing the command in Listing \ref{listing.hub}, although it is possible to assign additional configuration, such as port and IP address, using flags. The default port number is 4444 for hubs and 5555 for nodes, so if nothing else is specified, these ports will be used. When the hub has started, the configuration details for the hub itself and any connected nodes can be viewed by opening \url{http://<HubHost>:4444/grid/console} in a browser.

\vspace{4mm}
\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[caption=Sample Shell Command for Starting Selenium Grid Hub, label={listing.hub}, language=bash,   stringstyle={}]
$ java -jar "C:/selenium-server-standalone-2.51.0.jar"
\end{lstlisting}
\end{minipage}

Starting Selenium Grid nodes, or test machines, require much longer and heavier commands, and thus more work. Also, the command must be customized, as it represents the configuration of the node and the distributed system. Therefore, a script devoted to gathering all necessary information and executing the command to start Selenium Standalone Server is included in this project. This script identifies which browsers are installed and what versions, as well as creating a unique identifier for each machines.

Selenium Grid currently does not offer a documented method of specifying which machine a test should be executed on. Instead, it maps the test to a node whose configuration matches the desired specifications stated in the test, in regard to operating system, browser and sometimes even browser version. It was therefore necessary to find a way to work around this problem. This is done by utilizing a browser parameter called \emph{applicationName}, in which additional information can be added. A 128-bit unique identifier based on the host ID, sequence number, and the current time, is created using Python's \emph{UUID} (Universally Unique Identifier) library. By adding as a requirement to a test that the applicationName should be equal to the UUID of a specific node, the test can only be executed on the machine with that UUID.

\vspace{4mm}
\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[caption=Sample Shell Command for Starting Selenium Grid Node, label={listing.node}, language=bash,   stringstyle={}]
$ java -jar "C:/selenium-server-standalone-2.51.0.jar" -role node -hubHost <HubHost> -uuid 1b234276-fc02-11e5-b752-080027f8a664 -browser "browserName=chrome, version=49.0.2623.110, applicationName=1b234276-fc02-11e5-b752-080027f8a664" -Dwebdriver.chrome.driver=path/to/chromedriver.exe -browser "browserName=firefox, version=45.0, applicationName=1b234276-fc02-11e5-b752-080027f8a664"
\end{lstlisting}
\end{minipage}

Listing \ref{listing.node} shows an example of a command that will start a Selenium Grid Node with Chrome and Firefox installed, and with UUID \emph{1b234276-fc02-11e5-b752-080027f8a664}. Once the node is connected to the hub, the configuration can be retrieved as a JSON object from \url{http://<Hub Host>:4444/grid/api/proxy?id=http://<Node IP>:5555}.

\comment{\info[inline]{Maybe start by explaining exactly Selenium Grid works and is used. How the hub and the nodes are started (cmd), how platform and version no and installed browsers and version nos are retrieved and included, how the distributed environment works. How a JSON object is retrieved from "10.0.0.6:4444/grid/api/proxy?id=http://10.0.0.10:8989" or whatever to check node settings/abilities. Explain briefly how Selenium WebDriver works.}}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUBSECTION: Controller
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Controller}

The \emph{controller} consists of multiple processes running in different threads. Figure \ref{fig.controller_structure} shows the inner structure. Each of the modules in the figure will be explained in this section.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/controller4.pdf}
    \thisfloatpagestyle{plain}
    \caption{Controller Structure}
    \label{fig.controller_structure}
\end{figure}

\subsubsection{Selenium Server Listener}
After starting a Selenium Grid hub as described in Section \ref{section.selenium} using configuration details found in the configuration file, the main job of the Selenium Server Listener is to listen to the output from the Selenium Standalone Server. The server runs as a command line program and outputs short information and warning messages. \lstlistingname \space \ref{listing.hub_output} shows some sample output of the server when running as a hub. The listing covers start-up output as well as node registering, test execution and node unregistering. Each output message is to the Test Machine Manager, which looks for strings similar to the bold lines in the listing, to see whether a node has been registered or marked as down.


\vspace{4mm}
\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[caption=Sample Output from Selenium Standalone Server Running as a Hub, label={listing.hub_output}, language=bash, moredelim={[is][keywordstyle]{@@}{@@}}, deletekeywords={for, test}]
# Start-up
>>> 20:44:39.073 INFO - Launching Selenium Grid hub
>>> 20:44:41.646 INFO - Will listen on 4444
>>> 20:44:41.801 INFO - Nodes should register to http://<HubHost>:4444/grid/register/
>>> 20:44:41.802 INFO - Selenium Grid hub is up and running
>>> 20:44:59.072 WARN - Max instance not specified. Using default = 1 instance

# Node Registering
>>> @@20:44:59.094 INFO - Registered a node http://<NodeIP>:5555@@

# Test Execution
>>> 20:45:03.509 INFO - Got a request to create a new session: Capabilities [{browserName=internet explorer, applicationName=1b234276-fc02-11e5-b752-080027f8a664}]
>>> 20:45:03.510 INFO - Available nodes: [http://<NodeIP>:5555]
>>> 20:45:03.511 INFO - Trying to create a new session on node http://<NodeIP>:5555
>>> 20:45:03.511 INFO - Trying to create a new session on test slot {platform=WIN8_1, seleniumProtocol=WebDriver, browserName=internet explorer, applicationName=1b234276-fc02-11e5-b752-080027f8a664, version=9.11.9600.18321}

# Node Unregistering
>>> @@20:45:26.240 INFO - Marking the node http://<NodeIP>:5555 as down: cannot reach the node for 2 tries@@
>>> 20:46:26.607 INFO - Unregistering the node http://<NodeIP>:5555 because it\textquotesingles been down for 60367 milliseconds
>>> 20:46:26.607 WARN - Cleaning up stale test sessions on the unregistered node http://<NodeIP>:5555
\end{lstlisting}
\end{minipage}


\subsubsection{Test Machine Manager}

As the name suggests, the responsibility of the Test Machine Manager is to manage the test machines. Upon receiving a message from the Selenium Server Listener, the Test Machine Manager checks the content of the message to identify whether a machine has connected or disconnected from the grid. Configuration for newly connected machines is retrieved as a JSON object from the URL mentioned in Section \ref{section.selenium}. The specified UUID as well as information regarding installed browsers and operating system are all stored in the database.\comment{ As all test machines and their configuration other details are stored in the database, the any changes must be reported.}

There is a boolean \emph{Approved} attribute in the database which is used to manage which machines are allowed to be used for test executions. This attribute is set to \emph{False} by default for test machines that connects to the grid without being registered through the web interface. Test machine approval and disapproval can be managed from the dashboard.\improvement{TODO: implement as action. mention in ch 4.}

The Test Executor can request a list of all live test machines, in which case the Test Machine Manager queries the database for all approved test machines, and checks whether or not they're connected to the grid. This is done by checking the HTML code retrieved from \url{http://<HubHost>:4444/grid/console}, mentioned in Section \ref{section.selenium}. The MIT-licensed library \emph{Beautiful Soup} is then used to extract necessary information in order to construct a list of active, connected machines, from which non-approved machines are then removed.

a URL constructed by the IP address and the port number of the machine. Entering the URL of a Selenium Grid Node results in error code 403, so the test machine is only assumed live if the URL returns this error code.

\subsubsection{Request Listener}

All test executions managed by \toolname \space are either requested for immediate execution or scheduled ahead of time. There are two different modules in the controller each handling one of these.

The \emph{Request Listener} constantly listens to a port specified in the configuration file. When a user triggers the \emph{Execute Now} action from the dashboard, all necessary information about the tests being requested for execution is packed as a JSON object and sent to the controller using Python's \emph{socket} library, which communicates over TCP/IP \cite{https://docs.python.org/3/library/socket.html / https://docs.python.org/3/library/socket.html}.

Upon receiving a request, the request listener unpacks the JSON object and creates a list of corresponding \emph{test} objects. The list is then forwarded to the \emph{queue}.


\subsubsection{Schedule Listener}
When the Schedule Listener first starts, it retrieves schedule objects from the database. All schedule objects has a boolean \emph{Activated} attributes which defines whether the test cases in the schedule object should be executed according to schedule. Only activated schedule objects are retrieved from the database. The Schedule Listener then calculates if and when the next occurrence of each schedule object is due. The id and next occurrence time of all activated schedule objects with a next occurrence time are added to a list which the Schedule Listener uses to keep track of forthcoming test executions.

After the initial schedule retrieval, the Schedule Listener starts listening for schedule updates. As with immediate test execution requests, updates to schedule objects are also packed as JSON objects and sent over the network. An update is sent from the web when a schedule object is created, changed, or when the activation properties of a group of schedule objects are changed from the dashboard. The Schedule Listener receives and decodes the JSON objects, and updates the schedule list accordingly.

In an infinite while-loop in a separate thread, the Schedule Listener constantly checks whether the next occurrence of any schedule items are scheduled since the last check. If any schedule objects are due for execution, the test cases of the given schedule object are retrieved from the database, packed correctly and sent to the Test Executor. Then, the next occurrence of the schedule object is calculated, and the schedule list is updated accordingly.

\comment{
Overriding Django's built-in functionality for model deletion is a complex task, and was thus excluded, the Schedule Listener is not notified when a schedule object is deleted. The schedule is updated every 900 seconds, or 15 minutes.}

\subsubsection{Test Executor \& Queue System}

As test executions are requested by the controller, each individual test case is placed in an execution queue. The queuing system is made up of two distinct queues with descending priorities; one for immediate execution, which has the highest priority and is referred to as $Q_1$, and one for planned execution, which has the lowest priority and is referred to as $Q_2$. Which queue a test case is placed in is determined by which action triggered its execution request; if the request was received by the request listener, it is placed in $Q_1$, and if it was the schedule listener that identified it, it is placed in $Q_2$.

It is possible that a planned execution and an immediate execution of the same test case with the same browser and platform specifications is requested at the same time. A similar situation could occur when two schedule objects that are due at the same time includes the same test case with the same specifications. Scenarios such as these could potentially lead to time and resources wasted on performing the same job more than once. To avoid this, a duplicate check is performed each time a test case is added to one of the queues. If there are duplicates, the test case is removed from the queue with the lowest priority; $Q_2$.

The test executor always checks $Q_1$ first. If there are any test cases here, the test executor empties it and handles the tests. Otherwise, it checks $Q_2$, and does the same if there are any test cases waiting for execution in this queue. In other words, the test cases are not handled one by one, but in batches where the whole content of a queue is treated at once.

\emph{Starvation} is a condition in which some thread fails to make progress for an indefinite period of time \cite{opsys_boken}. As previously explained, it has been decided that immediate execution requests should have the highest priority. If some test cases were requested for immediate execution again and again while there were scheduled test cases in $Q_2$ waiting for execution, $Q_2$ would be blocked from making progress, as the prioritized queue, $Q_1$, would be populated and then emptied repeatedly. $Q_2$ would then experience starvation, which was an issue that needed to be addressed.

To work around the starvation problem, it was determined that on the occasion that $Q_1$ was emptied, any test cases currently located in $Q_2$ would be moved to $Q_1$. Thus in addition to avoiding starvation of $Q_2$, the requirement of immediate execution requests being prioritized is fulfilled, as they will always be placed in the queue that will be handled first. 

After test cases have been retrieved from the queue, allocated amongst the pool of available test machines (Section \ref{allocation}), and sorted, the execution can begin. A thread is started for each of the test machines. In these threads, each test case is started as a Python \emph{subprocess} using a shell command in which information regarding the desired test node and browser of the test execution are passed to the test script as arguments.

\comment{Immediately before a test case is executed, the test node is pinged to see if the connection is still up. This check is also done immediately after any failed test. If the node has crashed or otherwise failed before or during the execution, the test cases are moved to the queue for urgent executions, and attempted executed once the current test run has finished.}

If none of the test machines match the required browser/platform specification of a given test case, it will not be executed; the test will appear in the execution log, but will be marked as \emph{Not Executed}.





\subsection{Test Allocation Mechanism}

\improvement[inline]{TODO: Write some stuff here, including maybe some implementation details of ORX, and how the allocation mechanism integrates with the rest of the project. Change section title since there are a chapter with same name}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUBSECTION: Database
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Database}

The \emph{Relational Database Management System} (RDBMS) SQLite can be regarded as a light-weight substitute to other \emph{Structured Query Language} (SQL) based database engines. Benefits to SQLite compared to these other database systems include its being self-contained and serverless, and the database is contained in a single disk file \cite{https://www.sqlite.org/about.html}. For convenience concerning submission, SQLite was the preferred choice for this project. However, another SQL database engine can replace it with little effort if needed.

As explained in Chapter \ref{chapter.technology}, Django data models define the database layout, and each model typically maps to an individual table in the database. SQL statements for creating the database itself and the tables within it are all auto-generated by Django, based on the implementations of the models. If a many-to-many relation between two different models are defined in the model implementations, a separate relationship table is created in the database to cover this. The auto-generated SQL files covering creations and changes to the tables related to test automation are all placed under \emph{/testautomation/migrations/} by default. When a change has been done to one of the models, a new migration can be created by performing a few shell commands \cite{https://docs.djangoproject.com/en/1.9/topics/migrations/}.

The database can be accessed either by writing raw SQL queries, or through Django's API for database abstraction. The former approach was first implemented in this system, but was then changed to the latter, as it was cleaner and more consistent with the implementation of the rest of the project. It was also interesting to use a different practice of database communications than the more commonly used raw SQL queries. \lstlistingname \space \ref{listing.db} shows how an \emph{insert} statement is conducted in \toolname \space using this approach, although the listing is somewhat simplified as less attributes are specified. The implementation of the Log model is imported and then an object of this type is created with pseudo values for a small set of attributes. Line 5 in the listing represents the transaction execution and commit, where an entry in the database is created.

\vspace{4mm}
\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[caption=Database Communication Using Abstraction API, label={listing.db}, upquote=true]
from testautomation.models import Log
from datetime import datetime
 
l = Log(title='test1', result=True, execution_time=datetime.utcnow())
l.save()
\end{lstlisting}
\end{minipage}

In addition to providing excellent readability, the abstraction greatly decreases the required number of code lines needed to achieve the same result compared to executing raw SQL queries. This is because the database location does not need to be stated, connection with the database does not need to be programmatically established and then closed when the transaction is finished, etc. The abstraction takes care of all of this. Other types of queries such as \emph{select}, \emph{update} and \emph{delete} are also supported with this API.

\comment{
Information about test cases, test groups, schedules, logs as well as user information is all stored in various tables of the database. Figure \ref{fig.db_er} shows the Entity-Relationship (ER) diagram of the database.


\begin{figure}[p]
    \centering
    \thisfloatpagestyle{empty}
    \includegraphics[width=\textwidth,height=\textheight]{figures/er_diagram.pdf}
    \caption{ER Diagram of Database}
    \label{fig.db_er}
\end{figure}
}
All timestamps stored in the database are in the Coordinated Universal Time (UTC) standard, which, as the name suggests, is universal, and therefore independent of time zones. The time zone used in the web service is set to 'Europe/Oslo' in the Django settings file. Whenever a timestamp is shown on screen, it is first converted to the specified time zone using Django's \emph{timezone} library. If the user is located in a different time zone than the one specified in the settings, a label explaining that the computer time is a given number of hours ahead or behind of server time, is displayed next to any \emph{datetime} picker, such as in the schedule creation form.

Storing timestamps according to the UTC standard rather than the current time zone can be considered good practice for multiple reasons. Firstly, there can be no ambiguity. Confusion and misunderstandings related to conversion across different time zones will be avoided, which also means that timestamp calculations are simple. Further, there can be no invalid dates linked to daylight savings time. Moreover, if the server were to be moved to a different time zone, timestamps would have to be converted.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUBSECTION: Dashboard
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dashboard}

\improvement[inline]{TODO: Write section intro}
\comment{\todo[inline]{TODO: Describe maybe a little bit more how django/django admin works? Some parts are automatic and some are implemented. Some stuff are overrided, some stuff are extended, etc}}

\subsubsection{Models}

With Django, data models provide the foundation on which database tables are created and maintained. A model implementation can be seen as a Python equivalent to an SQL \emph{create table} statement. Model implementations are translated to SQL and executed by Django. Each model, which is a subclass of \emph{django.db.models.Model}, represents a table in the database, and each model field represents a database field. Similar to SQL, the data type of each field along with any other specifications such as the maximum length of a text field, default values and help text can be passed as \emph{field option} parameters.

The models are located in \emph{/testautomation/models.py}. In this file, model specifications of test cases, groups, schedules and logs are implemented.

Models can also have attributes derived from functions, which can use other attributes stored in the database. Derived attributes are used in several of the models. For instance in the test case module, there are one function that query the Log table in the database, and counts number of instances linked to the particular test case. Average duration and previous execution date are retrieved in a similar manner.

\vspace{4mm}
\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[caption=Model Implementation, label={listing.model}]
from django.db import models
 
class TestCase(models.Model):
    title           = models.CharField(max_length=80, null=False)
    script          = models.FileField(upload_to='scripts')
    description = models.TextField(null=True, blank=True)
    groups          = models.ManyToManyField('Group', blank=True)
    schedules       = models.ManyToManyField('Schedule', blank=True)
\end{lstlisting}
\end{minipage}

\lstlistingname \space \ref{listing.model} shows a reduced adaption of how the test case model has been implemented. This model contains two model fields and a function. The \emph{script} field is of the type \emph{model.FileField}, and the directory that the files should be uploaded to is passed as a parameter. The file upload itself is taken care of by Django.





\subsubsection{Admin}

\improvement[inline]{TODO: Write introduction to this subsection}

\vspace{4mm}
%\begin{minipage}{\textwidth}
\begin{lstlisting}[caption=Implementation of Model in Administrator Interface, label={listing.modeladmin}, float=bp, floatplacement=btp]
from django.contrib import admin
from .models import TestCase

class TestCaseAdmin(admin.ModelAdmin):
    list_display = ('title', 'times_run')
    search_fields = ['title',]
    fields = ('title', 'script')
    
    def times_run(self, obj):
        return Log.objects.filter(test_id=obj.pk).exclude(result__isnull=True).count()
    times_run.short_description = "Execution Count"
 
admin.site.register(TestCase, TestCaseAdmin)
\end{lstlisting}
%\end{minipage}

\lstlistingname \space \ref{listing.modeladmin} shows a simplified interpretation of how the test case model is represented in the administrator interface. This adaption builds on the model implementation from \lstlistingname \space \ref{listing.model}. Model administrator representations are subclasses of \emph{django.contrib.admin.ModelAdmin}, and specifies how the model should be represented in the administrator interface. This interpretation specifies values of three of the many ModelAdmin options; which fields of the model should be displayed in the overview list of the test case module (\emph{list\_display}), which fields should be searchable (\emph{search\_fields}) and which which fields should be present in the creation/change form (\emph{fields}). Line 9 registers the model to the administrator interface with the specifications stated in the TestCaseAdmin class.

In the actual implementations of the model admin representations, a number of additional fields and specifications are also included. Custom forms validation functionality can be integrated with the creation/change form. This has been done with test case objects to ensure that only test scripts that fulfill certain criteria can be uploaded. Admin actions are also implemented here.

\comment{\improvement{TODO: List and describe the implemented admin actions?}}

\subsubsection{Issue Tracker Reporting}

As earlier mentioned, Atlassin's JIRA is the issue tracker software used by Altibox. The issue tracker reporting supported by \toolname \space is therefore built on JIRA's \emph{REST API} (Representational State Transfer Application Program Interface) \cite{https://www.crummy.com/writing/RESTful-Web-Services/RESTful_Web_Services.pdf}.

The JIRA integration is used for two things: reporting failed test executions and retrieving issues linked to specific test executions in the execution log. These actions require access to the JIRA server used by Altibox, which is only available when connected to Altibox' \emph{Virtual Private Network} (VPN). To avoid problems in conjunction to VPN failure, an access check must be performed before proceeding. This is done by using the \emph{httplib} library to attempt establishing contact with the JIRA server and setting a 1 second timeout.

Failed test executions can be reported to JIRA by marking them in the log list and selecting \emph{Report to JIRA} from the actions menu. Any duplicates or log entries that did \emph{not} fail are removed from the list. JIRA access is checked. If the JIRA server is unavailable, an error message will be displayed. Otherwise, the REST API is used to search for JIRA issues linked to each log entry in the list. If there are any existing open issues for a log entry, a comment is added to the comment section of the issue, saying that the problem has been reproduced, and including details about the failed execution. Otherwise, a new issue is created.

On the log detail page, which is accessed by clicking on a log entry, there is a field displaying a list of clickable JIRA issues linked to the given test case, if any, and their respective statuses. As with issue reporting, an error message is shown if contact with the JIRA server could not be established.

\lstlistingname \space \ref{listing.jira} shows how issue search, commenting and creation are performed using the JIRA REST API. The Python code for this are located in \emph{/testautomation/admin.py}, and is called from the methods \emph{report\_to\_jira} and \emph{get\_jira\_issues} in the \emph{LogAdmin} class of this file.

\vspace{4mm}
%\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[caption={Issue search, commenting and creation using the \emph{JIRA REST API}}, label={listing.jira}, float=tp, floatplacement=tbp]
from jira import JIRA

jira_instance = JIRA(
    options={
        'server': jira_server,
        'verify': False,
        'get_server_info': False
    },
    basic_auth=('<Username>', '<Password>')
)

# JIRA Issue Search
issues = jira_instance.search_issues('<SQL search string>')
        
# JIRA Issue Commenting
jira_instance.add_comment(issues[0].id, '<Comment>')

# JIRA Issue Creation
jira_instance.create_issue(
    project='NGTV',
    summary='OptiRun: <Test Case Title> (<Test Case ID>) FAILED',
    description='<Issue Description>',
    issuetype={'name': 'Bug'},
    components=[{'name': 'web'}]
)
\end{lstlisting}
%\end{minipage}

\subsubsection{Event Recurrence}
\emph{RRULE} (Recurrence Rule) is a module in the Python library \emph{dateutil}, which provides an extension to Python's \emph{datetime} module. It  is a small and fast library used in \toolname \space to specify recurrence patterns of test executions. RRULE instances can be implemented multiple ways. In this project, it is achieved through passing a string with a specific format, containing information about the desired recurrence constraints. This string is stored in the database, and can be used at any point to create an RRULE instance in order to inquire when the next event should take place. An example of such a string, how RRULE instances are created in this project, and how the next occurrence is retrieved, can be seen in \lstlistingname \space \ref{listing.rrule}.

The string in the listing above is used to create an RRULE instance in which the first occurrence is set to the 15\textsuperscript{th} of June 2016 at 3 PM, and repeats weekly. The output shown in the listing is valid if the script was executed before this date, otherwise it will produce a different output  In addition to the recurrence properties shown in the listing above, the library provides an extensive number of recurrence options, including end date, occurrence count and interval.

\vspace{4mm}
\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[caption=Recursion Rule, label={listing.rrule}]
from dateutil.rrule import rrulestr
from datetime import datetime

rule_string = "DTSTART:20160615T150000\nRRULE:FREQ=WEEKLY"
rule = rrulestr(rule_string)

print rule.after(datetime.now())

>>> 2016-06-15 15:00:00
\end{lstlisting}
\end{minipage}

\comment{
    \missingfigure[figwidth=\textwidth]{Screenshot of filled-in schedule form? With corresponding rrulestr?}
    
    The recurrence rule strings in this project are built dynamically when a schedule object is created or updated. If the schedule creation form shown in Figure \ref{fig.rrule} is submitted, it would produce the following 
}

The RRULE strings in this project are built dynamically when a schedule object is created or edited, and then stored in the database as attributes to entries in the schedule table. In the test case, group and schedule modules of \toolname, recurrence rule strings are used to create RRULE instances, which again are used to find out the time of the next planned execution for the particular test case, group or schedule. RRULE instances are also created by the controller to check when the next test run is scheduled.


\comment{
\subsection{Other?}
\change[inline]{
Include data flow - what happens from someone clicks "Execute now" and until the result is shown in the log? (FLOW CHARTS!)
How are Jira issues created?

Include a (detailed?) figure system architecture of how the system is integrated. Start by explaining how the different parts talk to each other. In the beginning, consider the controller as a unit, give more detailed descriptions of objects, listeners, algorithms, etc later.

How does the scheduling algorithm work? Try to be as academic as possible - use academic language when describing the algorithm; define the problem mathematically, use graphs and flow charts, use an example.

Two types of scheduling:
- Optimal allocation and scheduling of test cases in a test run
- Planned scheduling of future test run with recurrence option (rrule, rrulestr)
}

\comment{SELENIUM GRID is load balanced, but otherwise chooses which machine to execute a test on sporadically. There is no documented way of specifying which node to execute a specific test on, but it can be done by adding some unique information to each node upon setup, and to use the same piece of information in the capability section in the setup function of the test script. https://groups.google.com/forum/#!topic/selenium-users/PRsEBcbpNlM}

\todo[inline]{
Scheduling algorithm:

job-machine-matrix / test case-node matrix (binary) (shows which machine can run each test case, which machines each test case can run on)}

    [
        \hfill
        \mathbb{M} = \kbordermatrix{
                  & tc_{1} & tc_{2} & tc_{3} &        & tc_{n} \\
            m_{1} & x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
            m_{2} & x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
                  & \vdots & \vdots & \vdots & \ddots & \vdots \\
            m_{d} & x_{d1} & x_{d2} & x_{d3} & \dots  & x_{dn}
        }
        \hfill
    ]



http://stackoverflow.com/questions/5674253/a-task-job-scheduling-problem

\comment{
\info[inline]{
Constraints:
- Each test case must be executed once and once only
- Machines can execute test cases in paralell
- Execution time is assumed to be machine independent
- Each machine can only execute one test case at a time
- Heterogeneous environment (at least not necessarily homogeneous) - different OS and OS versions, different browsers and browser versions - not all test cases can be executed on each machine

Objective: Minimizing running time


From TC-Sched:
time-constrained cumulative scheduling constraint-based technique because 1) it allows
us to keep fine-grained control on the time allocated to the
constraint solving process (i.e., time-constrained), 2) it encodes
exclusive resource use with constraints (i.e., constraint-based),
and 3) it solves the problem by using the CUMULATIVES constraint.
The TC-Sched method is composed of three elements,
namely, the constraint model described in Section III-A, the
search procedure described in Section III-B, and the timeconstrained
minimization process described in Section III-C.


Constraints:
-Non-cumulative scheduling: Two test cases cannot be executed at the same time on a single machine.
-Non-preemptive scheduling: The execution of a test case cannot be temporarily interrupted for the running machine to execute another test case instead.
-Non-shared resources: When a test case uses a global resource, no other test case using the same resource can be executed at the same time.
-Machine-independent execution time: The execution time of a test case is assumed to be independent of the machine on which it is run. This is a reasonable assumption for test cases in which the time is dominated by external physical factors such as a robotâ€™s motion, the opening of a valve, or sending an Ethernet frame. Such test cases typically have execution times that are uncorrelated with machine performance (e.g., CPU type, CPU frequency, operating system). In any case, a sufficient over-approximation of the execution time will satisfy the assumption.


EXPERIMENTAL EVALUATION

This section presents our findings from experimentally evaluating TC-Sched. To this end, we address the three following
research questions:

RQ1: How does the solution provided by TC-Sched compare with simpler scheduling methods in terms of the time needed to execute the schedule? RQ1 states the crucial question of wether using strong constraint optimisation tools is useful or
not in a context where cheaper-to-run methods are available at almost no cost of implementation.
RQ2: For TC-Sched, will an increased investment in the solving time reduce the overall time of a CI cycle? This question
is about finding the most appropriate trade-off between the solving time and the execution time of the test schedule.
RQ3: Can TC-Sched effectively handle industrial cases which contain up to hundreds of test cases and tens of machines?}
}
}